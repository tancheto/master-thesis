{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from fairlearn.adversarial import AdversarialFairnessClassifier\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from util import train_model_on_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "data_path = Path(os.getcwd()).parent.parent / \"data\" / \"dataset_diabetes\" / \"clsf_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = \"readmit_30_days\"\n",
    "sensitive_attribute = \"race\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(data_path / \"X_test_split.csv\")\n",
    "X_A_test = pd.read_csv(data_path / \"X_A_test_split.csv\")\n",
    "Y_test = pd.read_csv(data_path / \"Y_test_split.csv\")[target_variable]\n",
    "A_test = pd.read_csv(data_path / \"A_test_split.csv\")[sensitive_attribute]\n",
    "\n",
    "X_train = pd.read_csv(data_path / \"X_train_split.csv\")\n",
    "X_A_train = pd.read_csv(data_path / \"X_A_train_split.csv\")\n",
    "Y_train = pd.read_csv(data_path / \"Y_train_split.csv\")[target_variable]\n",
    "A_train = pd.read_csv(data_path / \"A_train_split.csv\")[sensitive_attribute]\n",
    "\n",
    "X_train_res_target_wos = pd.read_csv(data_path / \"X_train_res_target_wos.csv\")\n",
    "Y_train_res_target_wos = pd.read_csv(data_path / \"Y_train_res_target_wos.csv\")[target_variable]\n",
    "A_train_res_target_wos = pd.read_csv(data_path / \"A_train_res_target_wos.csv\").iloc[:, 0]\n",
    "\n",
    "X_A_train_res_target_ws = pd.read_csv(data_path / \"X_A_train_res_target_ws.csv\")\n",
    "Y_train_res_target_ws = pd.read_csv(data_path / \"Y_train_res_target_ws.csv\")[target_variable]\n",
    "A_train_res_target_ws = pd.read_csv(data_path / \"A_train_res_target_ws.csv\").iloc[:, 0]\n",
    "\n",
    "X_train_res_sensitive_wos = pd.read_csv(data_path / \"X_train_res_sensitive_wos.csv\")\n",
    "Y_train_res_sensitive_wos = pd.read_csv(data_path / \"Y_train_res_sensitive_wos.csv\")[target_variable]\n",
    "A_train_res_sensitive_wos = pd.read_csv(data_path / \"A_train_res_sensitive_wos.csv\").iloc[:, 0]\n",
    "\n",
    "X_A_train_res_sensitive_ws = pd.read_csv(data_path / \"X_A_train_res_sensitive_ws.csv\")\n",
    "Y_train_res_sensitive_ws = pd.read_csv(data_path / \"Y_train_res_sensitive_ws.csv\")[target_variable]\n",
    "A_train_res_sensitive_ws = pd.read_csv(data_path / \"A_train_res_sensitive_ws.csv\").iloc[:, 0]\n",
    "\n",
    "X_train_res_multiv_wos = pd.read_csv(data_path / \"X_train_res_multiv_wos.csv\")\n",
    "Y_train_res_multiv_wos = pd.read_csv(data_path / \"Y_train_res_multiv_wos.csv\")[target_variable]\n",
    "A_train_res_multiv_wos = pd.read_csv(data_path / \"A_train_res_multiv_wos.csv\").iloc[:, 0]\n",
    "\n",
    "X_A_train_res_multiv_ws = pd.read_csv(data_path / \"X_A_train_res_multiv_ws.csv\")\n",
    "Y_train_res_multiv_ws = pd.read_csv(data_path / \"Y_train_res_multiv_ws.csv\")[target_variable]\n",
    "A_train_res_multiv_ws = pd.read_csv(data_path / \"A_train_res_multiv_ws.csv\").iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Caucasian'],\n",
       "       ['Caucasian'],\n",
       "       ['Other'],\n",
       "       ...,\n",
       "       ['Caucasian'],\n",
       "       ['Caucasian'],\n",
       "       ['AfricanAmerican']], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_train.to_frame().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasDataSet(TensorDataset):\n",
    "\n",
    "    def __init__(self, *dataframes):\n",
    "        tensors = (self._df_to_tensor(df) for df in dataframes)\n",
    "        super(PandasDataSet, self).__init__(*tensors)\n",
    "\n",
    "    def _df_to_tensor(self, df):\n",
    "        if isinstance(df, pd.Series):\n",
    "            df = df.to_frame()\n",
    "        return torch.tensor(df.values).float()\n",
    "\n",
    "train_data = PandasDataSet(X_train, Y_train, pd.get_dummies(A_train))\n",
    "test_data = PandasDataSet(X_test, Y_test, pd.get_dummies(A_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training samples: 81410\n",
      "# batches: 2544\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "print('# training samples:', len(train_data))\n",
    "print('# batches:', len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, n_hidden=32, p_dropout=0.2):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_features, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_dropout),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_dropout),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_dropout),\n",
    "            nn.Linear(n_hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.sigmoid(self.network(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier(n_features=X_train.shape[1])\n",
    "clf_criterion = nn.BCELoss()\n",
    "clf_optimizer = optim.Adam(clf.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tanya\\Desktop\\TANYA\\FMI\\Masters\\thesis\\src\\env\\lib\\site-packages\\torch\\autograd\\__init__.py:251: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ..\\c10\\cuda\\CUDAFunctions.cpp:108.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "N_CLF_EPOCHS = 2\n",
    "\n",
    "for epoch in range(N_CLF_EPOCHS):\n",
    "    for x, y, _ in train_loader:\n",
    "        clf.zero_grad()\n",
    "        p_y = clf(x)\n",
    "        loss = clf_criterion(p_y, y)\n",
    "        loss.backward()\n",
    "        clf_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tanya\\Desktop\\TANYA\\FMI\\Masters\\thesis\\src\\env\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "class Adversary(nn.Module):\n",
    "\n",
    "    def __init__(self, n_sensitive, n_hidden=32):\n",
    "        super(Adversary, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(1, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_sensitive),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.sigmoid(self.network(x))\n",
    "\n",
    "lambdas = torch.Tensor([200, 30, 30, 30])\n",
    "adv = Adversary(n_sensitive=pd.get_dummies(A_train).shape[1])\n",
    "adv_criterion = nn.BCELoss(reduce=False)\n",
    "adv_optimizer = optim.Adam(adv.parameters())\n",
    "\n",
    "N_ADV_EPOCHS = 5\n",
    "\n",
    "for epoch in range(N_ADV_EPOCHS):\n",
    "    for x, _, z in train_loader:\n",
    "        adv.zero_grad()\n",
    "        p_y = clf(x).detach()\n",
    "        p_z = adv(p_y)\n",
    "        loss = (adv_criterion(p_z, z) * lambdas).mean()\n",
    "        loss.backward()\n",
    "        adv_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n"
     ]
    }
   ],
   "source": [
    "N_EPOCH_COMBINED = 165\n",
    "\n",
    "for epoch in range(1, N_EPOCH_COMBINED):\n",
    "    print(epoch)\n",
    "\n",
    "    # Train adversary\n",
    "    for x, y, z in train_loader:\n",
    "        adv.zero_grad()\n",
    "        p_y = clf(x)\n",
    "        p_z = adv(p_y)\n",
    "        loss_adv = (adv_criterion(p_z, z) * lambdas).mean()\n",
    "        loss_adv.backward()\n",
    "        adv_optimizer.step()\n",
    "\n",
    "    # Train classifier on single batch\n",
    "    for x, y, z in train_loader:\n",
    "        pass  # Ugly way to get a single batch\n",
    "    clf.zero_grad()\n",
    "    p_y = clf(x)\n",
    "    p_z = adv(p_y)\n",
    "    loss_adv = (adv_criterion(p_z, z) * lambdas).mean()\n",
    "    clf_loss = clf_criterion(p_y, y) - (adv_criterion(adv(p_y), z) * lambdas).mean()\n",
    "    clf_loss.backward()\n",
    "    clf_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator = AdversarialFairnessClassifier(\n",
    "    backend=\"torch\",\n",
    "    predictor_model=[50, \"leaky_relu\"],\n",
    "    adversary_model=[3, \"leaky_relu\"],\n",
    "    batch_size=2 ** 8,\n",
    "    progress_updates=0.5,\n",
    "    random_state=123,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator.fit(X_prep_train, Y_train, sensitive_features=Z_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
